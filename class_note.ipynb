{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# David Silver Reinforcement Learning Note\n",
    "\n",
    "## 目录\n",
    "\n",
    "- [Lecture 1 Introduction to Reinforcement Learning](#Lecture-1-Introduction-to-Reinforcement-Learning)\n",
    "- [Lecture 2 Markov Decision Processes](#Lecture-2-Markov-Decision-Processes)\n",
    "- [Lecture 3 Planning by Dynamic Programming](#Lecture-3-Planning-by-Dynamic-Programming)\n",
    "- [Lecture 4 Model Free Prediction](#Lecture-4-Model-Free-Prediction)\n",
    "- [Lecture 5 Model Free Control](#Lecture-5-Model-Free-Control)\n",
    "- [Lecture 6 Value Function Approximation](#Lecture-6-Value-Function-Approximation)\n",
    "- [Lecture 7 Policy Gradient](Lecture-7-Policy-Gradient)\n",
    "- [Lecture 8 Integrating Learning and Planning](Lecture-8-Integrating-Learning-and-Planning)\n",
    "- [Lecture 9 Exploration and Exploitation](Lecture-9-Exploration-and-Exploitation)\n",
    "- [Lecture 10 Classic Games](Lecture-10-Classic-Games)\n",
    "\n",
    "## 大致介绍\n",
    "\n",
    "介绍下课程推进的逻辑：\n",
    "\n",
    "第一章介绍基础概念，用着重分清楚`Reward`,`value function`和`q function`的区别。\n",
    "\n",
    "第二章介绍MDP，MDP是对环境的一种建模，即state将以一种什么样的方式推进是已知的。重点理解`Bellman Expectation Equation`和`Bellman Optimality Equation`，二者本质上都是递推公式，给动态规划打下基础。\n",
    "\n",
    "第三章介绍如何处理MDP，分为两部：`planning`和`controlling`。`planning`代表如何评估$v_{\\pi}$，`controlling`代表如何根据评估，找到最优的策略$\\pi_*$。解决方案包括`policy iteration`和`value iteration`。\n",
    "\n",
    "第四章介绍环境未知的情况下（model free），如何做planning。有两种方法：`MC（Monte-Carlo）`和`TD（Temporal Difference）`。\n",
    "\n",
    "第五章介绍环境未知的情况下，如何做`controlling`。针对policy的有：`On-Policy Monte-Carlo Control`，`On-Policy Sarsa Control`，`Off-Policy Q-learning`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 1 Introduction to Reinforcement Learning\n",
    "\n",
    "**奖励（Reward）**：$R_t$是$t$时刻的奖励。agent的任务是最大化**累积的（cumulative）**奖励。\n",
    "\n",
    "**观察（Observation）**：$O_t$，$t$时刻对环境的观察。\n",
    "\n",
    "**行动（Action）**：$A_t$，$t$时刻采取的行动。\n",
    "\n",
    "**历史（History）**：是一个序列的观察/奖励/行动（某时刻之前的所有时刻）。$H_t = A_1, O_1, R_1, ... A_t, O_t, R_t$。\n",
    "\n",
    "**状态（State）**：通过对历史信息的总结，决定接下来要发生什么。$S_t = f(H_t)$\n",
    "\n",
    "**环境状态（Environment State）**：某个环境的状态，$S_t^e$。\n",
    "\n",
    "**代理环境（Agent State）**：agent内部的环境，$S_t^a$。\n",
    "\n",
    "**马尔科夫状态（Markov state）**：状态$S_t$是马尔科夫状态，如果满足：$P[S_{t+1}|S_t] = P[S_{t+1}| S_1, ..., S_t]$，到这一步可以把**历史**丢掉了，只要每一步的状态即可。\n",
    "\n",
    "**全观测环境（Full observability）**：$O_t = S_t^a = S_t^e$。\n",
    "\n",
    "**部分观测环境（Partial observability）**：$S_t^a ≠ S_t^e$。\n",
    "\n",
    "**Policy**：Policy是agent的行为，是state到action的映射。确定形式：$a=\\pi (s)$；随机形式：$\\pi(a|s)=P[A_t=a|S_t=s]$。\n",
    "\n",
    "**Value**：Value是对未来reward的预测：$v_{\\pi}(s) = E_{\\pi}[R_{t+1}+\\gamma R_{t+2}+\\gamma ^2R_{t+3}+...|S_t=s]$\n",
    "\n",
    "**Model**：Model是对未来环境的预测：$\\textbf{P}$预测下一个state：$\\textbf{P}^a_{ss'}=P[S_{t+1}=s'|S_t=s,A_t=a]$，$\\textbf{R}$预测下一个即时的奖励：$\\textbf{R}_s^a=E[R_{t+1}|S_t=s,A_t=a]$。\n",
    "\n",
    "**Agent 分类**：\n",
    "- model based（model + policy and/or + value function）\n",
    "- model free:\n",
    "    - value based (value function)\n",
    "    - policy based (policy)\n",
    "    - actor critic (policy + value function)\n",
    "    \n",
    "![](https://github.com/applenob/rl_learn/raw/master/res/agent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 2 Markov Decision Processes\n",
    "\n",
    "**Markov Decision Process（MDP）**：思想：`The future is independent of the past given the present`，即某个时刻，agent所处在的状态，只和它上一个时刻所处的状态相关。是对**环境（实际上是state）**的一种描述。$P[S_{t+1}|S_t]=P[S_{t+1}|S_1,...,S_t]$\n",
    "\n",
    "**状态转移矩阵（State Transition Matrix）**：$\\textbf{P}$其中，$\\textbf{P}_{ss'}=P[S_{t+1}=s'|S_t=s]$。\n",
    "\n",
    "**Markov Process**：由简单到复杂：\n",
    "- 包含$<\\textbf{S}, \\textbf{P}>$，即**状态集合**和**状态转移矩阵**。\n",
    "- MRP：包含$<\\textbf{S}, \\textbf{P}, \\textbf{R}, \\gamma>$，即增加了**奖励函数**和**折扣系数**，$ \\textbf{R}_s = E[\\textbf{R}_{t+1} | S_t = s]$。\n",
    "- 引入$G_t=R_{t+1} + \\gamma R_{t+2} + ... =\\sum^{\\infty}_{k=0}\\gamma^kR_{t+k+1}$，注意Value是期望，G是针对一个sample，$v(s) = E [G_t | S_t = s]$。\n",
    "- MDP:包含$<\\textbf{S}, \\textbf{A}, \\textbf{P}, \\textbf{R}, \\gamma>$，增加了$\\textbf{A}$，是有限的动作集合。$\\textbf{P}_{ss'}^a=P[S_{t+1}=s'|S_t=s,A_t=a]$，$\\textbf{R}_s^a=E[R_{t+1}|S_t=s,A_t=a]$。\n",
    "\n",
    "**$G_t$ vs. $v(s)$**：再次注意二者的区分：$G_t$刻画的是针对一个sample序列（episode），在某一时刻$t$可以获得的长期奖励，这根据你的实验是可以计算出来的，因为你的试验中，每走一步会有一个$R_t$，$G_t=R_{t+1} + \\gamma R_{t+2} + ... =\\sum^{\\infty}_{k=0}\\gamma^kR_{t+k+1}$当然是可以计算的。$v_s$描述的是对某一个状态$s$的评价，我想不想经过这个状态。对状态的评价可以通过我在所有的episode中，只要状态是$s$的，我都计算一个$G_t$，然后把这些$G_t$求平均，就可以拿来评估这个状态的好坏，理论上，求平均应该是求期望。\n",
    "\n",
    "**MRP的Bellman Equation**：\n",
    "- 针对奖赏，有$$v(s) = E[G_t|S_t = s]\\\\=E[R_{t+1}+\\gamma v(S_{t+1})|S_t = s]\\\\=\\textbf{R}_s+\\gamma\\sum_{s'\\in S}\\textbf{P}_{ss'}v(s')$$\n",
    "- 矩阵形式：$v = R + \\gamma Pv$。\n",
    "- 直接求解，时间复杂度是$O(n^3)$。\n",
    "\n",
    "**Policy**：$\\pi$是给定状态之后动作的分布，即$\\pi(a|s)=P[A_t=a|S_t=s]$，policy可以完整地描述agent的行为，policy是独立于时间的。\n",
    "\n",
    "**MDP的state-value函数**：$v_{\\pi}(s)=E[G_t|S_t=s]$，即在状态$s$下，遵从policy-$\\pi$可以带来的长期奖励的期望。\n",
    "\n",
    "**MDP的action-value函数**：$q_{\\pi}(s,a) = E_{\\pi}[G_t|S_t=s, A_t=a]$，即在状态$s$下，采取了动作$a$，然后遵从policy-$\\pi$可以带来的长期奖励的期望。这是我们真正在乎的，我们需要知道哪个action更好！\n",
    "\n",
    "**q vs. v**：简单地说，$q$用来衡量某一个state下，采取某一个特定的action有多好，$v$用来衡量某一个state有多好。\n",
    "\n",
    "**MDP的Bellman Expectation Equation**：\n",
    "- $v_{\\pi}(s)=E_{\\pi}[R_{t+1}+\\gamma v_{\\pi}(S_{t+1})|S_t=s]$\n",
    "- $q_{\\pi}(s,a)=E_{\\pi}[R_{t+1}+\\gamma q_{\\pi}(S_{t+1}, A_{t+1})|S_t=s]$\n",
    "- $v_{\\pi}(s)=\\sum_{a\\in A}\\pi (a|s)q_{\\pi}(s,a)$\n",
    "- $q_{\\pi}(s,a)=\\textbf{R}_s^a+\\gamma\\sum_{s'\\in S}\\textbf{P}_{ss'}^av_{\\pi}(s')$\n",
    "- 最终：$v_{\\pi}(s)=\\sum_{a\\in A}\\pi (a|s)(\\textbf{R}_s^a+\\gamma\\sum_{s'\\in S}\\textbf{P}_{ss'}^av_{\\pi}(s'))$\n",
    "- 最终：$q_{\\pi}(s,a)=\\textbf{R}_s^a+\\gamma\\sum_{s'\\in S}\\textbf{P}_{ss'}^a\\sum_{a'\\in A}\\pi (a'|s')q_{\\pi}(s',a')$\n",
    "\n",
    "**MDP的Bellman Expectation Equation的矩阵形式**：\n",
    "- $v_{\\pi} = \\textbf{R}^{\\pi}+\\gamma\\textbf{P}^{\\pi}v_{\\pi}$\n",
    "\n",
    "**最优化的函数**：\n",
    "- $v_*(s)=\\underset{\\pi}{max}\\;v_{\\pi}(s)$\n",
    "- $q_*(s,a) = \\underset{\\pi}{max}\\;q_{\\pi}(s, a)$\n",
    "\n",
    "**policy的比较**：如果一个policy在所有state下，v都比比另一个policy大，那么可以说这个policy比另一个policy好。即，$\\pi > \\pi',\\;if\\;v_{\\pi}(s)≥v_{\\pi'}(s),\\forall s$\n",
    "\n",
    "**对于所有的MDP，都有**：\n",
    "- 存在最优policy，使得$\\pi_*≥\\pi,\\;\\forall \\pi$；\n",
    "- $v_{\\pi_*} = v_*(s)$；\n",
    "- $q_{\\pi_*}(s,a) = q_*(s,a)$。\n",
    "\n",
    "**假设已经找到$q_*(s,a)$，可以马上获得$\\pi_(a|s)$**：$\\pi_*(a|s)=\\left\\{\\begin{matrix}\n",
    "1\\;\\;if\\; a=\\underset{a\\in A}{argmax}\\;q_*(s,a)\\\\ \n",
    "0\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;otherwise\n",
    "\\end{matrix}\\right.$\n",
    "\n",
    "**Bellman Optimality Equation **：\n",
    "- for $v_*$：$v_*(s)=\\underset{a}{max}\\;q_*(s,a)$\n",
    "- for $q_*$：$q_*(s,a)=\\textbf{R}_s^a+\\gamma\\sum_{s'\\in S}\\textbf{P}_{ss'}^av_*(s')$\n",
    "\n",
    "**解决Bellman Optimality Equation的方法**：\n",
    "- Bellman Optimality Equation是非线性的，不能直接计算；\n",
    "- 迭代求解的方法：\n",
    "    - MDP：\n",
    "        - Value Iteration\n",
    "        - Policy Iteration\n",
    "    - Model Free：\n",
    "        - Q-learning\n",
    "        - Sarsa\n",
    "\n",
    "**Bellman Equation总结**：\n",
    "- Bellman Equation可以理解成某一时刻在某一状态下的value可以拆分成即时奖励和一下一个时刻开始所有可能的状态的价值的期望。\n",
    "- 它本身是一个递推公式，可以设计一个`backup`表格，不断进行更新。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 3 Planning by Dynamic Programming\n",
    "\n",
    "\n",
    "**动态规划（Dynamic Programming）**：\n",
    "- 给定MDP，用于MDP的**planning**；\n",
    "- **prediction**：\n",
    "    - 输入：MDP：$<\\textbf{S}, \\textbf{A}, \\textbf{P}, \\textbf{R}, \\gamma>$和policy：$\\pi$；或者MRP：$<\\textbf{S}, \\textbf{P}^{\\pi}, \\textbf{R}^{\\pi}, \\gamma>$。\n",
    "    - 输出：$v_{\\pi}$的值。\n",
    "- **control**:\n",
    "    - 输入：MDP：$<\\textbf{S}, \\textbf{A}, \\textbf{P}, \\textbf{R}, \\gamma>$。\n",
    "    - 输出：最优化的policy：$\\pi_*$\n",
    "\n",
    "**Iterative Policy Evaluation**：\n",
    "- 评估一个给定的policy：$\\pi$。\n",
    "- 使用Bellman Expectation Equation解决。\n",
    "- 迭代过程：$v_1\\to v_2\\to ...\\to v_{\\pi}$\n",
    "- 使用synchronous backups，在第$k+1$步迭代：\n",
    "    - 对所有的状态$s\\in S$\n",
    "    - 通过$v_k(s')$更新$v_{k+1}(s)$，矩阵形式：$\\textbf{v}^{k+1}=\\textbf{R}^{\\pi}+\\gamma\\textbf{P}^{\\pi}\\textbf{v}^k$，其中$s'$是$s$的后继状态。\n",
    "\n",
    "**Policy Iteration**：\n",
    "- 目标：寻找最优policy：$\\pi_*$。\n",
    "- 给定一个policy：$\\pi$：\n",
    "    - Evaluate这个policy：$v_{\\pi}(s)=E[G_t|S_t=s]=E[R_{t+1}+\\gamma R_{t+2}+...|S_t = s]$\n",
    "    - Improve这个policy：$\\pi'=greedy(v_{\\pi})$\n",
    "- 最终会收敛到$\\pi_*$。\n",
    "\n",
    "![](https://github.com/applenob/rl_learn/raw/master/res/policy_evaluation.png)\n",
    "\n",
    "**Value Iteration**：\n",
    "- 目标：寻找最优policy：$\\pi_*$。\n",
    "- 使用Bellman Optimality Equation解决。\n",
    "- 迭代过程：$v_1\\to v_2\\to ...\\to v_*$。\n",
    "- 使用synchronous backups，在第$k+1$步迭代：\n",
    "    - 对所有的状态$s\\in S$\n",
    "    - 通过$v_k(s')$更新$v_{k+1}(s)$，矩阵形式：$\\textbf{v}_{k+1}=\\underset{a \\in A}{max}\\;\\textbf{R}^a+\\gamma\\textbf{P}^a\\textbf{v}_k$。\n",
    "- 中间过程没有特定的policy。\n",
    "\n",
    "**Policy Iteration vs. Value Iteration**：\n",
    "- Policy Iteration: $v_1\\to \\pi \\to v_2$\n",
    "- Value Iteration：$v_1\\to v_2$\n",
    "- 中间的$v_i$可能没有对应的policy。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Lecture 4 Model Free Prediction\n",
    "\n",
    "**Monte-Carlo 强化学习**：\n",
    "- model free：没有mdp或者reward。\n",
    "- 适用于所有的episode都会结束的情形。\n",
    "\n",
    "**Monte-Carlo Policy Evaluation**：\n",
    "- Goal：使用policy：$\\pi$，从经验序列（episode）：$S_1,A_1,R_2,...,S_k$中学习价值$v_{\\pi}$。\n",
    "- 回顾$v_{\\pi}=E_{\\pi}(s) = E_{\\pi}[G_t|S_t=s]$，Monte-Carlo**使用平均返回值代替期望返回值**。\n",
    "- 有以下两种方法。\n",
    "\n",
    "**First-Vist Monte-Carlo Policy Evaluation**：\n",
    "- 评估某个状态$s$。\n",
    "- 在某一个episode中，第一次发生状态$s$的是时间$t$。\n",
    "- 计数器自增：$N(s) \\leftarrow  N(s)+1$\n",
    "- $S(s)\\leftarrow S(s) + G_t$\n",
    "- 计算价值函数：$V(s) \\leftarrow  S(s)/N(s)$\n",
    "\n",
    "**Every-Visit Monte-Carlo Policy Evaluation**：类似于上面，只不过是每次出现状态$s$，都要纳入计算。\n",
    "\n",
    "**增量计算均值**：$\\mu=\\frac{1}{k}\\sum_{j=1}^kx_j\\\\=\\frac{1}{k}(x_k+(k-1)\\mu_{k-1})\\\\=\\mu_{k-1}+\\frac{1}{k}(x_k-\\mu_{k-1})$\n",
    "\n",
    "**增量式的Monte-Carlo更新**：\n",
    "- 对于每个Episode：$S_1, A_1, R_2, ..., S_T$结束完以后，增量地更新$V(s)$。\n",
    "- $N(S_t) \\leftarrow N(S_t) + 1$\n",
    "- $V(S_t)\\leftarrow V(S_t)+\\frac{1}{N(S_t)}(G_t-V(S_t))$\n",
    "- 可以写成$V(S_t)\\leftarrow V(S_t)+\\alpha (G_t-V(S_t))$\n",
    "\n",
    "**瞬时分差Temporal-Difference Learning**：\n",
    "- model free：没有mdp或者reward。\n",
    "- 适用于不完整的episode的情形，使用bootstrapping（bootstrapping代表更新的时候需要估计期望，而不是使用真的反馈）。\n",
    "- TD通过猜测来更新猜测。\n",
    "\n",
    "**TD**：\n",
    "- Goal：使用policy：$\\pi$，中学习价值$v_{\\pi}$。\n",
    "- 相比MC只是更新的式子不同：$V(S_t)\\leftarrow V(S_t)+\\alpha (R_{t+1}+\\gamma V(S_{t+1})-V(S_t))$\n",
    "- 其中，$R_{t+1} + γV(S_{t+1})$ 被称为`TD target`。\n",
    "- $δ_t = R_{t+1} + γV(S_{t+1}) − V(S_t)$ 称为`TD error`。\n",
    "\n",
    "**MC vs. TD**：\n",
    "- 用通俗的语言描述就是，对于某个state的value更新，MC会把这个state后面所有的state都计算R，用来计算G；而TD只会计算当前下一个状态的R，其他后面的R不算，采用当前估计的value来替代。\n",
    "- MC有高方差，0偏差，对初始值不敏感，使用sample，不使用bootstrapping。\n",
    "- TD往往更高效，且对初始值敏感，使用sample，使用bootstrapping。\n",
    "\n",
    "**n-step TD**：\n",
    "![](https://github.com/applenob/rl_learn/raw/master/res/n_step_td.png)\n",
    "\n",
    "**TD($\\lambda$)**：\n",
    "- Forward-view：\n",
    "![](https://github.com/applenob/rl_learn/raw/master/res/td_lambda.png)\n",
    "- 每个n-step的$G_t$加权求和，权重：$(1-\\lambda)\\lambda^{n-1}$，$G_t^{\\lambda}=(1-\\lambda)\\sum^{\\infty}_{n=1}{\\lambda}^{n-1}G_t^{n}$\n",
    "- $V(S_t)\\leftarrow V(S_t) + \\alpha(G_t^{\\lambda}-V(S_t))$\n",
    "\n",
    "**Forward-view和Backward-view**：\n",
    "- Forward-view只能计算完整的episodes。\n",
    "- Backward-view可以计算不完整的episodes。\n",
    "- Forward-view提供了理论。\n",
    "- Backward-view提供了机制。\n",
    "- Forward-view倾向于最高频的状态。\n",
    "\n",
    "**Eligibility Traces**：\n",
    "- Eligibility Traces结合了高频的倾向和最近的倾向。\n",
    "- $E_0(s) = 0$\n",
    "- $E_t(s) = \\gamma\\lambda E_{t-1}(s) + \\textbf{1}(S_t=s)$，累加代表了高频倾向；但之前的时刻会衰减代表了最近的倾向。\n",
    "\n",
    "**Backward-view TD($\\lambda$)**：\n",
    "- $\\delta_t=R_{t+1}+\\gamma V(S_{t+1} - V(S_t))$\n",
    "- $V(s)\\leftarrow V(s)+\\alpha \\delta_t E_t(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Lecture 5 Model Free Control\n",
    "\n",
    "**On-policy learning vs. Off-policy learning**：\n",
    "- on代表在任务中学习，从采样的$\\pi$中学习$\\pi$。（自学）\n",
    "- off代表在某种基础上学习，从采样的$\\mu$中学习$\\pi$。（有人示范）\n",
    "\n",
    "**On-Policy Monte-Carlo Control**：\n",
    "- policy evaluation：Monte-Carlo policy evaluation，$Q≈q_{\\pi}$。\n",
    "- policy improvement：$\\epsilon$-greedy policy improvement\n",
    "![](https://github.com/applenob/rl_learn/raw/master/res/mc_policy_iteration.png)\n",
    "\n",
    "**为什么使用Q？**：V的`greedy policy improvement`依赖于MDP，而Q的则不需要。\n",
    "\n",
    "**$\\epsilon$-greedy policy improvement**：\n",
    "- $\\pi(a|s)=\\left\\{\\textbf{}\\begin{matrix}\\epsilon/m+1-\\epsilon\\;\\;\\;if\\;a*=\\underset{a\\in A}{argmax}Q(s,a)\\\\ \\epsilon/m\\;\\;\\;otherwise\\end{matrix}\\right.$\n",
    "- 即，有$\\epsilon$的可能是随机选择。\n",
    "\n",
    "**Greedy in the Limit with Infinite Exploration (GLIE)**：\n",
    "- 希望上面提到的$\\epsilon$是衰减的，即到最后的时候，直接采用贪婪选择。\n",
    "\n",
    "**On-Policy Sarsa Control**：\n",
    "![](https://github.com/applenob/rl_learn/raw/master/res/sarsa_2.png)\n",
    "![](https://github.com/applenob/rl_learn/raw/master/res/sarsa.png)\n",
    "- 所谓sarsa即，使用$\\epsilon$-greedy，根据$s$，选择$a$，获得奖励$r$，和新的状态$s'$，再使用$\\epsilon$-greedy，选择$a'$\n",
    "\n",
    "**n-step Sarsa**：\n",
    "![](https://github.com/applenob/rl_learn/raw/master/res/n_step_sarsa.png)\n",
    "\n",
    "**Forward View Sarsa($\\lambda$)**：\n",
    "- $q_t^{\\lambda}=(1-\\lambda)\\sum_{n=1}^{\\infty}\\lambda ^{n-1}q_t^{(n)}$\n",
    "- $Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha(q_t^{\\lambda}-Q(S_t, A_t))$\n",
    "\n",
    "**Backward View Sarsa($\\lambda$)**：\n",
    "- $E_0(s, a)= 0$\n",
    "- $E_t(s, a)=\\gamma\\lambda E_{t-1}(s, a)+\\mathbf{1}(S_t=s,A_t=a)$\n",
    "- $\\delta_t = R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)$\n",
    "- $Q(s, a)\\leftarrow Q(s, a)+\\alpha\\delta_tE_t(s,a)$\n",
    "![](https://github.com/applenob/rl_learn/raw/master/res/sarsa_lamda.png)\n",
    "\n",
    "**Off-Policy Learning**：behaviour policy：$\\mu(a|s)$\n",
    "\n",
    "**Q-Learning**：\n",
    "- off-policy。\n",
    "- 不需要importance采样。\n",
    "- 下一个动作从behaviour policy中选择：$A_{t+1}\\sim \\mu(\\cdot|S_t)$\n",
    "- 备选的动作：$A'\\sim \\pi(\\cdot|S_t)$，用于下面式子的计算。\n",
    "- 更新$Q(S_t, A_t)\\leftarrow Q(S_t, A_t)+\\alpha(R_{t+1}+\\gamma Q(S_{t+1}, A')-Q(S_t, A_t))$\n",
    "![](https://github.com/applenob/rl_learn/raw/master/res/q-learning.png)\n",
    "\n",
    "**MDP vs. Model Free**：\n",
    "![](https://github.com/applenob/rl_learn/raw/master/res/dp_td.png)\n",
    "![](https://github.com/applenob/rl_learn/raw/master/res/dp_td2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Lecture 6 Value Function Approximation\n",
    "\n",
    "**对于大型的MDP**：\n",
    "- 之前我们通过一张大表，表格上有$V(s)$或者$Q(s,a)$。\n",
    "- 对于大型的MDP问题，表格不再适用，于是可以估计价值函数。\n",
    "- $\\hat v(s, w) \\approx v_{\\pi}(s)$\n",
    "- $\\hat q(s,a,w) \\approx q_{\\pi}(s, a)$\n",
    "- 使用MC或者TD来更新$w$。\n",
    "\n",
    "**Value Function Approximation的种类**：\n",
    "![](https://github.com/applenob/rl_learn/raw/master/res/fun_app.png)\n",
    "\n",
    "**随机梯度下降**：\n",
    "- 先假设有个已知的$v_{\\pi}(S)$\n",
    "- 目标：$J(w) = E_{\\pi}[(v_{\\pi}(S)-\\hat v(S,w))^2]$\n",
    "- 梯度下降：$\\triangle w = -\\frac{1}{2}\\alpha\\triangledown_wJ(w)=\\alpha E_{\\pi}[(v_{\\pi}(S)-\\hat v(S,w))\\triangledown_w \\hat v(S,w)]$\n",
    "- 随机梯度下降只用sample其中一个：$\\triangle w =\\alpha (v_{\\pi}(S)-\\hat v(S,w))\\triangledown_w \\hat v(S,w)$\n",
    "\n",
    "**特征向量（Feature Vector）**：\n",
    "- 用来表示一个state。\n",
    "- $x(S) = \\begin{pmatrix}x_1(S)\\\\ ...\\\\ x_n(S)\\end{pmatrix}$\n",
    "\n",
    "**线性特征组合**：\n",
    "- 用特征的线性组合去表示value function。\n",
    "- $\\hat v(S, w) = x(S)^Tw=\\sum^n_{j=1}x_j(S)w_j$\n",
    "- $\\triangle w = \\alpha(v_{\\pi}(S)-\\hat v(S, w))x(S)$\n",
    "- 也就是 update = 步长×预测误差×特征值\n",
    "\n",
    "**增量预测算法（Incremental Prediction Algorithms）**：\n",
    "- 回到“$v_{\\pi}(S)$是未知的”这个事实。\n",
    "- 我们使用以下的方法，不同的方法使用不同的量去替代$v_{\\pi}(S)$。\n",
    "- $MC$：使用$G_t$，$\\triangle w = \\alpha(G_t - \\hat v(S_t, w))\\triangledown_w\\hat v(S_t, w)$\n",
    "- $TD(0)$：使用$R_{t+1}+\\gamma \\hat v(S_{t+1}, w)$，$\\triangle w = \\alpha(R_{t+1}+\\gamma \\hat v(S_{t+1}, w) - \\hat v(S_t, w))\\triangledown_w\\hat v(S_t, w)$\n",
    "- $TD(\\lambda)$：使用$G_t^{\\lambda}$，$\\triangle w = \\alpha(G_tt^{\\lambda} - \\hat v(S_t, w))\\triangledown_w\\hat v(S_t, w)$\n",
    "\n",
    "**Control with Value Function Approximation**：\n",
    "![](https://github.com/applenob/rl_learn/raw/master/res/control_VFA.png)\n",
    "\n",
    "**Action-Value Function Approximation**：和Value Function Approximation类似。\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Lecture 7 Policy Gradient\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 8 Integrating Learning and Planning\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 9 Exploration and Exploitation\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Lecture 10 Classic Games\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****：\n",
    "\n",
    "****："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
