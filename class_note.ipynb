{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# David Silver Reinforcement Learning Note\n",
    "\n",
    "- [Lecture 1 Introduction to Reinforcement Learning](#Lecture-1-Introduction-to-Reinforcement-Learning)\n",
    "- [Lecture 2 Markov Decision Processes](#Lecture-2-Markov-Decision-Processes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 1 Introduction to Reinforcement Learning\n",
    "\n",
    "**奖励（Reward）**：$R_t$是$t$时刻的奖励。agent的任务是最大化**累积的（cumulative）**奖励。\n",
    "\n",
    "**观察（Observation）**：$O_t$，$t$时刻对环境的观察。\n",
    "\n",
    "**行动（Action）**：$A_t$，$t$时刻采取的行动。\n",
    "\n",
    "**历史（History）**：是一个序列的观察/奖励/行动（某时刻之前的所有时刻）。$H_t = A_1, O_1, R_1, ... A_t, O_t, R_t$。\n",
    "\n",
    "**状态（State）**：通过对历史信息的总结，决定接下来要发生什么。$S_t = f(H_t)$\n",
    "\n",
    "**环境状态（Environment State）**：某个环境的状态，$S_t^e$。\n",
    "\n",
    "**代理环境（Agent State）**：agent内部的环境，$S_t^a$。\n",
    "\n",
    "**马尔科夫状态（Markov state）**：状态$S_t$是马尔科夫状态，如果满足：$P[S_{t+1}|S_t] = P[S_{t+1}| S_1, ..., S_t]$，到这一步可以把**历史**丢掉了，只要每一步的状态即可。\n",
    "\n",
    "**全观测环境（Full observability）**：$O_t = S_t^a = S_t^e$。\n",
    "\n",
    "**部分观测环境（Partial observability）**：$S_t^a ≠ S_t^e$。\n",
    "\n",
    "**Policy**：Policy是agent的行为，是state到action的映射。确定形式：$a=\\pi (s)$；随机形式：$\\pi(a|s)=P[A_t=a|S_t=s]$。\n",
    "\n",
    "**Value**：Value是对未来reward的预测：$v_{\\pi}(s) = E_{\\pi}[R_{t+1}+\\gamma R_{t+2}+\\gamma ^2R_{t+3}+...|S_t=s]$\n",
    "\n",
    "**Model**：Model是对未来环境的预测：$\\textbf{P}$预测下一个state：$\\textbf{P}^a_{ss'}=P[S_{t+1}=s'|S_t=s,A_t=a]$，$\\textbf{R}$预测下一个即时的奖励：$\\textbf{R}_s^a=E[R_{t+1}|S_t=s,A_t=a]$。\n",
    "\n",
    "**Agent 分类**：\n",
    "- model based（model + policy and/or + value function）\n",
    "- model free:\n",
    "    - value based (value function)\n",
    "    - policy based (policy)\n",
    "    - actor critic (policy + value function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 2 Markov Decision Processes\n",
    "\n",
    "**Markov Decision Process（MDP）**：思想：`The future is independent of the past given the present`。是对**环境（实际上是state）**的一种描述。\n",
    "\n",
    "**状态转移矩阵（State Transition Matrix）**：$\\textbf{P}$其中，$\\textbf{P}_{ss'}=P[S_{t+1}=s'|S_t=s]$。\n",
    "\n",
    "**Markov Process**：由简单到复杂：\n",
    "- 包含$<\\textbf{S}, \\textbf{P}>$，即状态集合和状态转移矩阵。\n",
    "- MRP：包含$<\\textbf{S}, \\textbf{P}, \\textbf{R}, \\gamma>$，即增加了奖励函数和折扣系数，$ \\textbf{R}_s = E[\\textbf{R}_{t+1} | S_t = s]$。\n",
    "- 引入$G_t=R_{t+1} + \\gamma R_{t+2} + ... =\\sum^{\\infty}_{k=0}\\gamma^kR_{t+k+1}$，注意Value是期望，G是针对一个sample，$v(s) = E [G_t | S_t = s]$。\n",
    "- MDP:包含$<\\textbf{S}, \\textbf{A}, \\textbf{P}, \\textbf{R}, \\gamma>$，增加了$\\textbf{A}$，是有限的动作集合。$\\textbf{P}_{ss'}^a=P[S_{t+1}=s'|S_t=s,A_t=a]$，$\\textbf{R}_s^a=E[R_{t+1}|S_t=s,A_t=a]$。\n",
    "\n",
    "**MRP的Bellman Equation**：\n",
    "- 针对奖赏，有$$v(s) = E[G_t|S_t = s]\\\\=E[R_{t+1}+\\gamma v(S_{t+1})|S_t = s]\\\\=\\textbf{R}_s+\\gamma\\sum_{s'\\in S}\\textbf{P}_{ss'}v(s')$$\n",
    "- 矩阵形式：$v = R + \\gamma Pv$。\n",
    "- Bellman Equation可以理解成某一时刻在某一状态下的价值可以拆分成即时奖励和一下一个时刻开始所有可能的状态的价值的期望。\n",
    "- 直接求解，时间复杂度是$O(n^3)$。\n",
    "\n",
    "**Policy**：$\\pi$是给定状态之后动作的分布，即$\\pi(a|s)=P[A_t=a|S_t=s]$，policy可以完整地描述agent的行为，policy是独立于时间的。\n",
    "\n",
    "**MDP的state-value函数**：$v_{\\pi}(s)=E[G_t|S_t=s]$，即在状态$s$下，遵从policy$\\pi$可以带来的长期奖励的期望。\n",
    "\n",
    "**MDP的action-value函数**：$q_{\\pi}(s,a) = E_{\\pi}[G_t|S_t=s, A_t=a]$，即在状态$s$下，采取了动作$a$，然后遵从policy$\\pi$可以带来的长期奖励的期望。这是我们真正在乎的，我们需要知道哪个action更好！\n",
    "\n",
    "**q和v**：简单地说，$q$用来衡量采取某一个特定的action有多好，$v$用来衡量某一个state有多好。\n",
    "\n",
    "**MDP的Bellman Expectation Equation**：\n",
    "- $v_{\\pi}(s)=E_{\\pi}[R_{t+1}+\\gamma v_{\\pi}(S_{t+1})|S_t=s]$\n",
    "- $q_{\\pi}(s,a)=E_{\\pi}[R_{t+1}+\\gamma q_{\\pi}(S_{t+1}, A_{t+1})|S_t=s]$\n",
    "- $v_{\\pi}(s)=\\sum_{a\\in A}\\pi (a|s)q_{\\pi}(s,a)$\n",
    "- $q_{\\pi}(s,a)=\\textbf{R}_s^a+\\gamma\\sum_{s'\\in S}\\textbf{P}_{ss'}^av_{\\pi}(s')$\n",
    "- 最终：$v_{\\pi}(s)=\\sum_{a\\in A}\\pi (a|s)(\\textbf{R}_s^a+\\gamma\\sum_{s'\\in S}\\textbf{P}_{ss'}^av_{\\pi}(s'))$\n",
    "- 最终：$q_{\\pi}(s,a)=\\textbf{R}_s^a+\\gamma\\sum_{s'\\in S}\\textbf{P}_{ss'}^a\\sum_{a'\\in A}\\pi (a'|s')q_{\\pi}(s',a')$\n",
    "\n",
    "**MDP的Bellman Expectation Equation的矩阵形式**：$v_{\\pi} = \\textbf{R}^{\\pi}+\\gamma\\textbf{P}^{\\pi}v_{\\pi}$\n",
    "\n",
    "**最优化的函数**：\n",
    "- $v_*(s)=\\underset{\\pi}{max}\\;v_{\\pi}(s)$\n",
    "- $q_*(s,a) = \\underset{\\pi}{max}\\;q_{\\pi}(s, a)$\n",
    "\n",
    "**policy的比较**：如果一个policy在所有state下，v都比比另一个policy大，那么可以说这个policy比另一个policy好。即，$\\pi > \\pi',\\;if\\;v_{\\pi}(s)≥v_{\\pi'}(s),\\forall s$\n",
    "\n",
    "**对于所有的MDP，都有**：\n",
    "- 存在最优policy，使得$\\pi_*≥\\pi,\\;\\forall \\pi$；\n",
    "- $v_{\\pi_*} = v_*(s)$；\n",
    "- $q_{\\pi_*}(s,a) = q_*(s,a)$。\n",
    "\n",
    "**假设已经找到$q_*(s,a)$，可以马上获得$\\pi_(a|s)$**：$\\pi_*(a|s)=\\left\\{\\begin{matrix}\n",
    "1\\;\\;if\\; a=\\underset{a\\in A}{argmax}\\;q_*(s,a)\\\\ \n",
    "0\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;otherwise\n",
    "\\end{matrix}\\right.$\n",
    "\n",
    "**Bellman Optimality Equation **：\n",
    "- for $v_*$：$v_*(s)=\\underset{a}{max}\\;q_*(s,a)$\n",
    "- for $q_*$：$q_*(s,a)=\\textbf{R}_s^a+\\gamma\\sum_{s'\\in S}\\textbf{P}_{ss'}^av_*(s')$\n",
    "\n",
    "**解决Bellman Optimality Equation的方法**：\n",
    "- Bellman Optimality Equation是非线性的，不能直接计算；\n",
    "- 迭代求解的方法：\n",
    "    - Value Iteration\n",
    "    - Policy Iteration\n",
    "    - Q-learning\n",
    "    - Sarsa\n",
    "\n",
    "****："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
