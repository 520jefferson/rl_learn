{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 强化学习指南\n",
    "\n",
    "## 经验心得\n",
    "\n",
    "## 地图\n",
    "\n",
    "- 1.理解强化学习的几大要素：`policy`，`reward function`，`value function`，`modle (of the environment)`，分清`Reward`,`value function`和`q function`的区别。\n",
    "\n",
    "- 2.理解MDP的概念，MDP是对环境的一种建模，能覆盖绝大多数的强化学习问题。\n",
    "    - `Bellman Expectation Equation`：$v_{\\pi}(s) = \\sum_a\\pi(a|s)\\sum_{s',r}p(s',r|s,a)[r+\\gamma v_{\\pi}(s')]\\;\\;\\forall s \\in S$\n",
    "    - `Bellman Optimality Equation`：$v_*(s)=\\underset{a\\in A(s)}{max}\\sum_{s',r}p(s',r|s,a)[r+\\gamma v_*(s')]$和$q_*(s,a)=\\sum_{s',r}p(s',r|s,a)[r+\\gamma \\underset{a'}{max}q_*(s', a')]$\n",
    "    - 二者本质上都是递推公式，其中蕴含的**“backup”**思路，给动态规划打下基础。\n",
    "\n",
    "- 3.有了MDP的概念，接下来考虑如何解决MDP的问题。\n",
    "    - `planning`代表如何评估$v_{\\pi}$，\n",
    "    - `controlling`代表如何根据评估，找到最优的策略$\\pi_*$。\n",
    "        - `policy iteration`：每轮迭代先评估policy，获得新的$v$；再根据新的$v$，修改policy。$v_{\\pi}(s) = \\sum_a\\pi(a|s)\\sum_{s',r}p(s',r|s,a)[r+\\gamma v_{\\pi}(s')]\\;\\;\\forall s \\in S$\n",
    "        - `value iteration`：每轮迭代直接寻找当轮最优的$v$，所有迭代完再找到对应的最优policy。$v_{k+1}(s)=\\underset{a}{max} E[R_{t+1}+\\gamma v_k(S_{t+1})|S_t=s, A_t=a]\\\\=\\underset{a}{max}\\sum_{s',r}p(s',r|s,a)[r+\\gamma v_k(s')]$\n",
    "\n",
    "- 4.接下来考虑model free，即没有状态转移概率。\n",
    "    - `planning`：\n",
    "        - `MC（Monte-Carlo）`：每次生成一条完整的episode，然后，对某个状态$s$计算平均$G$作为对$v$的估计。$V(s)\\leftarrow average(Return(s))$\n",
    "        - `TD（Temporal Difference）`：每做一次决定，都可以更新$v$：$V(S)\\leftarrow V(S)+\\alpha[R+\\gamma V(S')-V(S)]$\n",
    "    - `controlling`：\n",
    "        - `On-Policy Monte-Carlo Control`：`MC planning`+`greedy policy`\n",
    "        - `On-Policy Sarsa Control`：`TD planning`+`greedy policy`，但是更新从$v$变成了$q$，因此叫`SARSA`：$Q(S,A)\\leftarrow Q(S,A)+\\alpha[R+\\gamma Q(S',A')-Q(S,A)]$\n",
    "        - Off-Policy：在生成episode的时候，直接使用一种较优的policy去指导它，而不是最终的`target policy`，指导的policy称为`behavior policy`。\n",
    "        - `Off-Policy Monte-Carlo Control`：相比`On-Policy Monte-Carlo Control`y加入重要性采样。\n",
    "        - `Off-Policy Q-learning`：相比于`Sarsa`，$Q(S,A)\\leftarrow Q(S,A)+\\alpha[R+\\gamma Q(S',A')-Q(S, A)]$\n",
    "\n",
    "- 5.近似函数：\n",
    "    - 近似价值函数：目标$J(w) = E_{\\pi}[(v_{\\pi}(S)-\\hat v(S,w))^2]$，使近似的价值函数接近实际的价值函数。\n",
    "        - `Q-Learning with Linear Function Approximation`\n",
    "        - `Deep-Q Learning（DQN）`：使用了`Experience Replay`和`fixed Q-learning target`。\n",
    "    - 拟合策略函数：目标$J_1(\\theta)=V^{\\pi_{\\theta}}(s_1) = E_{\\pi_{\\theta}}[v_1]$，使找到的策略函数可以使价值函数最大化。\n",
    "        - `Monte-Carlo Policy Gradient (REINFORCE)`\n",
    "    - 近似价值函数 + 拟合策略函数\n",
    "        - ` Actor-Critic`：Critic：更新价值函数的参数$w$ 。Actor：更新策略的参数 $θ$ ，使用critic建议的方向。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
